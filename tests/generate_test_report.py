#!/usr/bin/env python3
"""
Test Report Generator for YAMY Automated Testing
=================================================

Generates HTML reports from automated test results with:
- Color-coded pass/fail visualization
- Detailed statistics and comparison to baseline
- Specific failure details for debugging
- Layer-by-layer analysis

Usage:
    python3 generate_test_report.py [--input RESULTS_FILE] [--output REPORT.html] [--baseline BASELINE_RATE]
"""

import json
import sys
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass


@dataclass
class TestResult:
    """Test result from automated_keymap_test.py"""
    input_key: str
    output_key: str
    input_evdev: int
    output_evdev: int
    event_type: str
    passed: bool
    expected_evdev: int
    actual_evdev: Optional[int]
    error_message: Optional[str]


class TestReportGenerator:
    """Generates HTML test reports from test results."""

    def __init__(self, baseline_pass_rate: float = 50.0):
        """
        Initialize report generator.

        Args:
            baseline_pass_rate: Baseline pass rate percentage for comparison
        """
        self.baseline_pass_rate = baseline_pass_rate
        self.results: List[TestResult] = []
        self.stats: Dict[str, any] = {}

    def load_results_from_json(self, json_path: str) -> bool:
        """
        Load test results from JSON file.

        JSON format:
        {
            "stats": {
                "total_tests": 174,
                "passed": 150,
                "failed": 24,
                "pass_rate": 86.2,
                "substitutions_tested": 87
            },
            "results": [
                {
                    "input_key": "W",
                    "output_key": "A",
                    "input_evdev": 17,
                    "output_evdev": 30,
                    "event_type": "PRESS",
                    "passed": true,
                    "expected_evdev": 30,
                    "actual_evdev": 30,
                    "error_message": null
                },
                ...
            ]
        }

        Args:
            json_path: Path to JSON results file

        Returns:
            True if loaded successfully, False otherwise
        """
        try:
            with open(json_path, 'r') as f:
                data = json.load(f)

            self.stats = data.get('stats', {})

            # Load results
            results_data = data.get('results', [])
            self.results = []

            for r in results_data:
                self.results.append(TestResult(
                    input_key=r['input_key'],
                    output_key=r['output_key'],
                    input_evdev=r['input_evdev'],
                    output_evdev=r['output_evdev'],
                    event_type=r['event_type'],
                    passed=r['passed'],
                    expected_evdev=r['expected_evdev'],
                    actual_evdev=r.get('actual_evdev'),
                    error_message=r.get('error_message')
                ))

            return True

        except Exception as e:
            print(f"ERROR: Failed to load results from {json_path}: {e}")
            return False

    def parse_text_results(self, text: str) -> bool:
        """
        Parse results from text format (fallback if JSON not available).

        This is a simple parser for the text report format generated by
        automated_keymap_test.py.

        Args:
            text: Text report content

        Returns:
            True if parsed successfully, False otherwise
        """
        # For now, just extract basic stats from text
        # This is a fallback - JSON format is preferred
        import re

        # Extract statistics
        total_match = re.search(r'Total Tests:\s*(\d+)', text)
        passed_match = re.search(r'PASSED:\s*(\d+)', text)
        failed_match = re.search(r'FAILED:\s*(\d+)', text)
        rate_match = re.search(r'PASS RATE:\s*([\d.]+)%', text)

        if total_match and passed_match and failed_match and rate_match:
            self.stats = {
                'total_tests': int(total_match.group(1)),
                'passed': int(passed_match.group(1)),
                'failed': int(failed_match.group(1)),
                'pass_rate': float(rate_match.group(1)),
                'substitutions_tested': int(total_match.group(1)) // 2
            }
            return True

        return False

    def generate_html_report(self, output_path: str, config_file: str = None) -> bool:
        """
        Generate HTML report.

        Args:
            output_path: Path to output HTML file
            config_file: Path to .mayu config file being tested

        Returns:
            True if generated successfully, False otherwise
        """
        if not self.stats:
            print("ERROR: No test statistics loaded")
            return False

        html = self._build_html(config_file)

        try:
            with open(output_path, 'w') as f:
                f.write(html)
            print(f"‚úì HTML report generated: {output_path}")
            return True
        except Exception as e:
            print(f"ERROR: Failed to write HTML report: {e}")
            return False

    def _build_html(self, config_file: Optional[str]) -> str:
        """Build complete HTML report."""
        return f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YAMY Automated Test Report</title>
    <style>
        {self._get_css()}
    </style>
</head>
<body>
    <div class="container">
        {self._build_header(config_file)}
        {self._build_summary()}
        {self._build_baseline_comparison()}
        {self._build_failures_section()}
        {self._build_footer()}
    </div>
</body>
</html>"""

    def _get_css(self) -> str:
        """Get CSS styles for the report."""
        return """
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            background: #f5f5f5;
            padding: 20px;
            line-height: 1.6;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.5em;
        }

        h2 {
            color: #34495e;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 1.8em;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }

        h3 {
            color: #555;
            margin-top: 20px;
            margin-bottom: 10px;
            font-size: 1.3em;
        }

        .header-meta {
            color: #7f8c8d;
            font-size: 0.95em;
            margin-bottom: 30px;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .stat-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            text-align: center;
            border-left: 4px solid #3498db;
        }

        .stat-card.passed {
            border-left-color: #27ae60;
        }

        .stat-card.failed {
            border-left-color: #e74c3c;
        }

        .stat-card.rate {
            border-left-color: #f39c12;
        }

        .stat-value {
            font-size: 2.5em;
            font-weight: bold;
            color: #2c3e50;
            display: block;
            margin-bottom: 5px;
        }

        .stat-label {
            color: #7f8c8d;
            font-size: 0.9em;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .comparison-box {
            background: #e8f4f8;
            border: 2px solid #3498db;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
        }

        .comparison-box.improved {
            background: #d5f4e6;
            border-color: #27ae60;
        }

        .comparison-box.degraded {
            background: #fadbd8;
            border-color: #e74c3c;
        }

        .comparison-value {
            font-size: 1.5em;
            font-weight: bold;
            color: #2c3e50;
        }

        .improvement-indicator {
            font-size: 1.2em;
            font-weight: bold;
            display: inline-block;
            margin-left: 10px;
        }

        .improvement-indicator.positive {
            color: #27ae60;
        }

        .improvement-indicator.negative {
            color: #e74c3c;
        }

        .failure-list {
            margin: 20px 0;
        }

        .failure-item {
            background: #fff5f5;
            border-left: 4px solid #e74c3c;
            padding: 15px;
            margin-bottom: 15px;
            border-radius: 4px;
        }

        .failure-header {
            font-size: 1.1em;
            font-weight: bold;
            color: #c0392b;
            margin-bottom: 10px;
        }

        .failure-details {
            display: grid;
            grid-template-columns: auto 1fr;
            gap: 10px 20px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            color: #555;
        }

        .failure-label {
            font-weight: bold;
            color: #7f8c8d;
        }

        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #ddd;
            text-align: center;
            color: #7f8c8d;
            font-size: 0.9em;
        }

        .pass-indicator {
            color: #27ae60;
            font-weight: bold;
        }

        .fail-indicator {
            color: #e74c3c;
            font-weight: bold;
        }

        .inconclusive-indicator {
            color: #f39c12;
            font-weight: bold;
        }

        .no-failures {
            background: #d5f4e6;
            border: 2px solid #27ae60;
            border-radius: 6px;
            padding: 30px;
            text-align: center;
            font-size: 1.2em;
            color: #27ae60;
            margin: 20px 0;
        }

        .no-failures::before {
            content: "‚úì";
            font-size: 3em;
            display: block;
            margin-bottom: 10px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.95em;
        }

        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }

        th {
            background: #f8f9fa;
            font-weight: 600;
            color: #2c3e50;
        }

        tr:hover {
            background: #f8f9fa;
        }
        """

    def _build_header(self, config_file: Optional[str]) -> str:
        """Build report header."""
        return f"""
        <h1>YAMY Automated Test Report</h1>
        <div class="header-meta">
            <strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}<br>
            <strong>Config File:</strong> {config_file or 'N/A'}<br>
            <strong>Spec:</strong> key-remapping-consistency (Phase 3, Task 3.6)
        </div>
        """

    def _build_summary(self) -> str:
        """Build test summary section."""
        total = self.stats.get('total_tests', 0)
        passed = self.stats.get('passed', 0)
        failed = self.stats.get('failed', 0)
        pass_rate = self.stats.get('pass_rate', 0.0)
        substitutions = self.stats.get('substitutions_tested', 0)

        return f"""
        <h2>Test Summary</h2>
        <div class="stats-grid">
            <div class="stat-card">
                <span class="stat-value">{total}</span>
                <span class="stat-label">Total Tests</span>
            </div>
            <div class="stat-card passed">
                <span class="stat-value">{passed}</span>
                <span class="stat-label">Passed</span>
            </div>
            <div class="stat-card failed">
                <span class="stat-value">{failed}</span>
                <span class="stat-label">Failed</span>
            </div>
            <div class="stat-card rate">
                <span class="stat-value">{pass_rate:.1f}%</span>
                <span class="stat-label">Pass Rate</span>
            </div>
        </div>
        <p style="color: #7f8c8d; margin-top: 15px;">
            Tested <strong>{substitutions}</strong> substitutions with both PRESS and RELEASE events
            ({substitutions} √ó 2 = {total} total tests).
        </p>
        """

    def _build_baseline_comparison(self) -> str:
        """Build baseline comparison section."""
        current_rate = self.stats.get('pass_rate', 0.0)
        improvement = current_rate - self.baseline_pass_rate

        if improvement > 0:
            comparison_class = "improved"
            indicator_class = "positive"
            indicator_symbol = "‚Üë"
        elif improvement < 0:
            comparison_class = "degraded"
            indicator_class = "negative"
            indicator_symbol = "‚Üì"
        else:
            comparison_class = ""
            indicator_class = ""
            indicator_symbol = "="

        return f"""
        <h2>Baseline Comparison</h2>
        <div class="comparison-box {comparison_class}">
            <div>
                <strong>Baseline Pass Rate (Task 1.6):</strong>
                <span class="comparison-value">{self.baseline_pass_rate:.1f}%</span>
            </div>
            <div style="margin-top: 10px;">
                <strong>Current Pass Rate:</strong>
                <span class="comparison-value">{current_rate:.1f}%</span>
                <span class="improvement-indicator {indicator_class}">
                    {indicator_symbol} {abs(improvement):.1f}%
                </span>
            </div>
            <p style="margin-top: 15px; color: #555;">
                {self._get_comparison_message(improvement)}
            </p>
        </div>
        """

    def _get_comparison_message(self, improvement: float) -> str:
        """Get comparison message based on improvement."""
        if improvement >= 40:
            return "üéâ <strong>Excellent!</strong> Dramatic improvement achieved through EventProcessor refactoring."
        elif improvement >= 20:
            return "‚úì <strong>Significant improvement</strong> - refactoring is working well."
        elif improvement >= 10:
            return "‚úì <strong>Good progress</strong> - continued improvement needed."
        elif improvement >= 0:
            return "‚ö† <strong>Minor improvement</strong> - additional fixes required."
        else:
            return "‚ùå <strong>Regression detected</strong> - investigate immediately."

    def _build_failures_section(self) -> str:
        """Build failures section."""
        failures = [r for r in self.results if not r.passed]

        if not failures:
            return """
            <h2>Test Failures</h2>
            <div class="no-failures">
                ALL TESTS PASSED!
                <div style="margin-top: 10px; font-size: 0.9em;">
                    100% pass rate achieved - excellent work!
                </div>
            </div>
            """

        # Group failures by type
        no_output = [f for f in failures if f.actual_evdev is None]
        wrong_output = [f for f in failures if f.actual_evdev is not None]

        html = f"""
        <h2>Test Failures</h2>
        <p>Found <strong>{len(failures)}</strong> failed tests. Details below:</p>
        """

        if no_output:
            html += f"""
            <h3>No Output Detected ({len(no_output)} failures)</h3>
            <p style="color: #7f8c8d; margin-bottom: 15px;">
                These tests produced no output in the logs. Possible causes: YAMY not running,
                YAMY_DEBUG_KEYCODE not set, or events not reaching Layer 3.
            </p>
            <div class="failure-list">
            """

            for failure in no_output[:10]:  # Limit to first 10
                html += self._build_failure_item(failure)

            if len(no_output) > 10:
                html += f"""
                <div style="margin: 15px 0; color: #7f8c8d; font-style: italic;">
                    ... and {len(no_output) - 10} more failures with no output
                </div>
                """

            html += "</div>"

        if wrong_output:
            html += f"""
            <h3>Wrong Output ({len(wrong_output)} failures)</h3>
            <p style="color: #7f8c8d; margin-bottom: 15px;">
                These tests produced output but with incorrect evdev codes. Indicates layer processing errors.
            </p>
            <div class="failure-list">
            """

            for failure in wrong_output:
                html += self._build_failure_item(failure)

            html += "</div>"

        return html

    def _build_failure_item(self, failure: TestResult) -> str:
        """Build HTML for a single failure item."""
        actual_display = failure.actual_evdev if failure.actual_evdev is not None else "N/A"

        return f"""
        <div class="failure-item">
            <div class="failure-header">
                {failure.input_key} ‚Üí {failure.output_key} ({failure.event_type})
            </div>
            <div class="failure-details">
                <span class="failure-label">Input evdev:</span>
                <span>{failure.input_evdev}</span>

                <span class="failure-label">Expected evdev:</span>
                <span>{failure.expected_evdev} ({failure.output_key})</span>

                <span class="failure-label">Actual evdev:</span>
                <span>{actual_display}</span>

                <span class="failure-label">Error:</span>
                <span>{failure.error_message or 'Unknown error'}</span>
            </div>
        </div>
        """

    def _build_footer(self) -> str:
        """Build report footer."""
        return f"""
        <div class="footer">
            Generated by YAMY Automated Test Report Generator<br>
            Task 3.6: Test Report Generator | Spec: key-remapping-consistency
        </div>
        """


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Generate HTML test reports from YAMY test results",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Generate report from JSON results
    python3 generate_test_report.py --input results.json --output report.html

    # Generate report with custom baseline
    python3 generate_test_report.py --input results.json --baseline 45.0

    # Specify config file for report header
    python3 generate_test_report.py --input results.json --config keymaps/config_clean.mayu
        """
    )

    parser.add_argument(
        '--input',
        help='Path to test results file (JSON format preferred)'
    )

    parser.add_argument(
        '--output',
        default='test_report.html',
        help='Path to output HTML file (default: test_report.html)'
    )

    parser.add_argument(
        '--baseline',
        type=float,
        default=50.0,
        help='Baseline pass rate for comparison (default: 50.0)'
    )

    parser.add_argument(
        '--config',
        help='Path to .mayu config file being tested (for report header)'
    )

    parser.add_argument(
        '--stats-only',
        action='store_true',
        help='Generate report from statistics only (no detailed results)'
    )

    parser.add_argument(
        '--total',
        type=int,
        help='Total number of tests (for stats-only mode)'
    )

    parser.add_argument(
        '--passed',
        type=int,
        help='Number of passed tests (for stats-only mode)'
    )

    parser.add_argument(
        '--failed',
        type=int,
        help='Number of failed tests (for stats-only mode)'
    )

    args = parser.parse_args()

    # Create report generator
    generator = TestReportGenerator(baseline_pass_rate=args.baseline)

    # Load results
    if args.stats_only:
        # Stats-only mode
        if not all([args.total is not None, args.passed is not None, args.failed is not None]):
            print("ERROR: --stats-only mode requires --total, --passed, and --failed")
            sys.exit(1)

        generator.stats = {
            'total_tests': args.total,
            'passed': args.passed,
            'failed': args.failed,
            'pass_rate': (args.passed / args.total * 100) if args.total > 0 else 0,
            'substitutions_tested': args.total // 2
        }

    elif args.input:
        # Load from file
        if args.input.endswith('.json'):
            if not generator.load_results_from_json(args.input):
                sys.exit(1)
        else:
            # Try parsing as text
            with open(args.input, 'r') as f:
                text = f.read()
            if not generator.parse_text_results(text):
                print("ERROR: Failed to parse results from input file")
                print("Hint: JSON format is preferred. See automated_keymap_test.py for format.")
                sys.exit(1)
    else:
        print("ERROR: Either --input or --stats-only mode is required")
        parser.print_help()
        sys.exit(1)

    # Generate report
    if not generator.generate_html_report(args.output, args.config):
        sys.exit(1)

    # Print summary
    print("\n" + "=" * 80)
    print("REPORT GENERATED SUCCESSFULLY")
    print("=" * 80)
    print(f"Pass Rate: {generator.stats['pass_rate']:.1f}%")
    print(f"Output: {args.output}")
    print("\nOpen the HTML file in a browser to view the full report.")


if __name__ == '__main__':
    main()
